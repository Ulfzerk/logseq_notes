- Statistic is **sufficient** with respoect to a statistical models and its associated unknow parameter if
	- "no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter"
	- In fact there is a function $T(X)$ where X is a sample of data, which allows to compute any estimate of the parameter (e.g. a maximum likelihood estimate).
	- Then we call call $T(X)$ a sufficient statistic in respect to \theta (parameter of model)
	- For example for a \mu in normal distribution we can use mean of data (which is a function of data sample)
	- It can lead to data reduction which is nice ;)
	- It is inpractical to use it from definition because it would be a guessing of function $T(X)$
- ### Neyman-Fisher Theorem (Factorization Criterion)
	- It is a practical tool for identification of sufficient statistics
	- if the probability density function is $f_{\theta} (x) = h(x)g_\theta (T(X))
	- the density f can be factored into a product such that one factor, h, does not depend on \theta and the other factor, which does depend on \theta, depends on x only through T(x).
	- General proof -  Halmos and Savage
	-
	-
- #### Resource:
	- http://prac.im.pwr.wroc.pl/~wilczyn/wstep%20do%20statystyki/wyklad%207.pdf
	-